# Линейная фильтрация изображений (вертикальное разбиение). Ядро Гаусса 3x3.

- Студент: Отческов Семён Андреевич, группа 3823Б1ПР1
- Технологии: SEQ | MPI
- Вариант: 27

## 1. Введение
- Линейная фильтрация изображений является одной из основных тем, изучаемых в компьютерном зрении и обработке изображений. Процесс заключается в применении фильтра (ядра свёртки) к изображению. Одним из таких фильтров является ядро Гаусса, применяемый для сглаживания и устранения шума изображения.
- В данной работе представлены два алгоритма линейной фильтрации изображений с использованием ядра Гаусса 3×3: последовательная (базовая) реализация и параллельная реализация с вертикальным разбиением изображения и использованием технологии MPI (Message Passing Interface).

- **Цель работы:** сравнение производительности алгоритмов и анализ эффективности распараллеливания вычислительной задачи при обработке изображений различного размера.

## 2. Постановка задачи
**Формальная постановка:**
- Для каждого пикселя изображения *I* с координатами *(x,y)* и для каждого цветового канала *c* вычислить новое значение путем свертки с гауссовским ядром 3x3: 
  - $I_{out}\left(x, y, c\right)=\sum\limits_{i=-1}^{1}{\sum\limits_{j=-1}^{1}{I_{mirrored}\left(x+i,y+j,c\right) \cdot K\left(i,j\right)}}$
  - $k\left(i, j\right)$ — веса ядра Гаусса
  - $I_{mirrored}$ — изображение с обработанными границами методом отражения (reflection)
- Ядро Гаусса 3x3 имеет следующий веса:
  - $K=\frac{1}{16}\ast\begin{bmatrix}
      1 & 2 & 1  \\
      2 & 4 & 2 \\
      1 & 2 & 1 
      \end{bmatrix}$

**Входные данные:**
- Изображение в виде трехмерного массива (высота × ширина × количество каналов).
- Значения пикселей представлены целыми числами в диапазоне [0, 255].
- Поддерживаются изображения с различным количеством цветовых каналов (1 для градаций серого, 3 для RGB и т.д.).

**Выходные данные:**
- Отфильтрованное изображение того же размера и формата, что и входное.

**Ограничения:**
- Количество цветовых каналов должно быть положительным целым числом.

## 3. Описание алгоритма (последовательного)

### 3.1. Этапы выполнения задачи
**1. Валидация данных (`ValidationImpl`):**
- Проверка на пустоту входного изображения.
- Проверка минимальных размеров изображения (не менее $3×3$ пикселей).
- Проверка корректности количества цветовых каналов (должно быть больше нуля).
- Проверка соответствия объема данных заявленным размерам изображения.

**2. Предобработка данных (`PreProcessingImpl`):**
- Задача не требует предобработки, поэтому данный этап пропускается.

**3. Вычисления (`RunImpl`):**
- Выделение памяти для выходного изображения с сохранением размеров входного.
- Последовательный обход всех пикселей изображения по строкам и столбцам:
  - Для каждого пикселя и каждого цветового канала:
    - Инициализация суммы взвешенных значений.
    - Перебор всех 9 элементов ядра Гаусса (окно $3×3$).
    - Для каждой позиции ядра вычисление координат соответствующего пикселя с учетом обработки границ `методом отражения`.
    - Умножение значения пикселя на соответствующий вес из ядра Гаусса и добавление к сумме.
  - Округление итоговой суммы и ограничение диапазона значений [0, 255].
  - Запись результата в выходное изображение.

- Метод отражения:
  - Если индекс выходиз за верхнюю границу ($y<0$), то $y=-y-1$.
  - Если индекс выходит за нижнюю границу ($y\geqslant height$), то $y=2*height-y-1$.

**4. Постобработка данных (`PostProcessingImpl`):**
- Задача не требует постобработки, поэтому данный этап пропускается.

### 3.2. Сложность алгоритма:
- Временная сложность: $O(H \cdot W \cdot C)$
  - $H$ — высота изображения.
  - $W$ — ширина изображения.
  - $C$ — количество каналов изображения.
  - Для каждого пикселя выполняется фиксированное число операций — 9 умножений и сложений в случае ядра Гаусса $3×3$.
- Пространственная сложность: $O(H \cdot W \cdot C)$ — хранение входного и выходного изображений..

## 4. Схема распараллеливания алгоритма с помощью MPI

### 4.1. Распределение данных
- Вертикальное разбиение изображения:
  - Процесс ранга 0 распределяет данные между всеми процессами с помощью `MPI_Scatterv`
  - Размер количество столбцов (`base_cols`) определяется как `ширина изображения / число активных процессов`.
  - Если ширина изображения меньше числа процессов, используются только `active_procs_ = min(proc_num_, width)` процессов.
  - Если есть остаток после делении числа столбцов, то он добавляется по одному столбцу первым `reminder` процессам.
  - Каждый процесс получает вертикальную полосу изображения в локальный буфер `local_data`.
  - Размер локальной полосы для процесса: `local_width_ = base_cols + (proc_rank_ < remainder ? 1 : 0)`.

### 4.2. Топология коммуникаций
- Линейная топология с соседними связями:
  - все процессы связаны через `MPI_COMM_WORLD`.
  - Дополнительный связи формируются динамически для обмена граничными столбцами.
- Роль процессов:
  - **Ранг 0**: координатор — вычисляет распределение, рассылает метаданные, распределяет данные, собирает результаты, обрабатывает левую границу изображения.
  - **Ранг N-1 (active_procs_ - 1)**: обрабатывает правую границу изображения.
  - **Промежуточные ранги**: Обмениваются данными с левыми и правыми соседями для формирования расширенной области, что поспособствует меньшим обращениям к процессам при фильтрации границ своих локальных частей.

### 4.3. Паттерны коммуникации
- Функция `MPI_Bcast`:
  - Применяется в валидации, чтобы каждый процесс прошёл проверку аналогично процессу ранга 0.
  - Применяется для рассылки метаданных об изображении.
- Функция `MPI_Scatterv`:
  - Распределение вертикальных полос изображения с учётом неравномерного деления.
  - Каждый процесс получает свою часть данных согласно своему числу обрабатываемых элементов и смещению в глобальном массиве.
- Функция `MPI_Sendrecv`:
  1. Двусторонний обмен граничными столбцами между соседними процессами.
  2. Каждый процесс одновременно отправляет левый столбец левому соседу и правый столбец правому соседу.
  3. Используются разные теги сообщений (0 для левого обмена, 1 для правого) для избежания конфликтов.
- Функция `MPI_Gatherv`:
  - Асимметричный сбор обработанных полос на процессе 0.
  - Учитывает неравномерное распределение столбцов при формировании итогового изображения.

### 4.4. Распределение вычислений
1. **Инициализация MPI** — получение ранга и числа процессов.
2. **Определение распределения** — вычисление размеров блоков и смещений.
3. **Распределение данных** — рассылка частей вектора на процессы.
4. **Локальные вычисления**— суммирование элементов локального сегмента.
5. **Глобальная редукция** — объединение локальных сумм.
6. **Финальное вычисление** — расчёт среднего значения.

## 5. Особенности реализаций

### 5.1. Структура кода
Реализации классов и методов на языке С++ указаны в [Приложении](#10-приложение).
```
tasks/otcheskov_s_gauss_filter_ver_split/
├── common
│ └── include
│     └── common.hpp
├── data
│   ├── chess.jpg
│   └── gradient.jpg
├── mpi
│   ├── include
│   │   └── ops_mpi.hpp
│   └── src
│       └── ops_mpi.cpp
├── seq
│   ├── include
│   │   └── ops_seq.hpp
│   └── src
│       └── ops_seq.cpp
├── tests
│   ├── functional
│   │   └── functional.cpp
│   └── performance
│       └── performance.cpp
├── report.md
├── settings.json
└── info.json
    
```
#### 5.1.1. Файлы
- `./common/include/common.hpp` — общие определения типов данных ([см. Приложение №1](#101-приложение-1--общие-определения)).
- `./seq/include/ops_seq.hpp` — определение класса последовательной версии задачи ([см. Приложение №2.1](#1021-заголовочный-файл)).
- `./seq/include/ops_mpi.hpp` — определение класса параллельной версии задачи ([см. Приложение №2.2](#1021-файл-реализации)).
- `./seq/src/ops_seq.cpp` — реализация последовательной версии задачи ([см. Приложение №3.1](#1031-заголовочный-файл)).
- `./seq/src/ops_mpi.cpp` — реализация параллельной версии задачи ([см. Приложение №3.2](#1032-файл-реализации)).
- `./tests/functional/main.cpp` — реализация функциональных и валидационных тестов ([см. Приложение №4](#104-приложение-4--функциональные-и-валидационные-тесты)).
- `./tests/performance/main.cpp` — реализация производительных тестов ([см. Приложение №5](#105-приложение-5--проиводительные-тесты)).

#### 5.1.2. Ключевые классы
- `OtcheskovSGaussFilterVertSplitSEQ` — последовательная версия.
- `OtcheskovSGaussFilterVertSplitMPI` — параллельная версия.
- `OtcheskovSGaussFilterVertSplitValidationTests` — валидационные тесты.
- `OtcheskovSGaussFilterVertSplitFuncTests` — функциональные тесты.
- `OtcheskovSGaussFilterVertSplitRealTests` — тесты с реальными изображениями.
- `OtcheskovSGaussFilterVertSplitPerfTests` — производительные тесты.

#### 5.1.3. Основные методы
- `ValidationImpl` — валидация входных данных и состояния выходных данных.
- `PreProcessingImpl` — препроцессинг, не используется.
- `RunImpl` — основная логика вычислений.
- `PostProcessingImpl` — постпроцессинг, не используется.

### 5.2. Реализация последовательной версии

#### 5.2.1. ValidationImpl
- Проверка непустоты массива пикселей `data`.
- Проверка минимальных размеров изображения (не менее $3×3$ пикселей).
- Проверка положительного количества цветовых каналов.
- Соответсвие объёма данных `data` с заявленными в метаданных размерами (`data.size() == height * width * channels`).
- Результат проверки кэшируется в поле `is_valid_` для последующего использования в `RunImpl`, для корректной работы тестов.

**Реализация на C++:**
```c++
bool OtcheskovSGaussFilterVertSplitSEQ::ValidationImpl() {
  const auto &[metadata, data] = GetInput();
  is_valid_ = !data.empty() && 
             (metadata.height >= 3 && metadata.width >= 3 && metadata.channels > 0) &&
             (data.size() == metadata.height * metadata.width * metadata.channels);
  return is_valid_;
}
```

#### 5.2.2. PreProcessingImpl
В препроцессинге нет необходимости, поэтому данный этап пропускается.

#### 5.2.3. RunImpl
1. Проверка валидности данных:
  - Используется закэшированное значение `is_valid` из этапа валидации.
2. Подготовка выходного буфера:
  - Создаётся массив пикселей того же размера, что и входной.
3. Обработка каждого пикселя:
  - Для каждого пикселя `(row, col)` и каждого цветового канала `ch`:
    - Инициализация суммы взвешенных значений.
    - Применяется окно $3×3$ с использованием ядра Гаусса:
      ```cpp
      const auto process_pixel = [&](size_t row, size_t col, size_t ch) -> uint8_t {
        double sum = 0.0;
        for (int dy = -1; dy <= 1; ++dy) {
          size_t src_y = MirrorCoord(row, dy, height);
          for (int dx = -1; dx <= 1; ++dx) {
            size_t src_x = MirrorCoord(col, dx, width);
            double weight = kGaussianKernel.at(dy + 1).at(dx + 1);
            size_t src_idx = GetIndex(src_y, src_x, ch);
            sum += weight * in_data[src_idx];
          }
        }
        return static_cast<uint8_t>(std::clamp(std::round(sum), 0.0, 255.0));
      };
      ```
    - Обработка границ через отражение координат функцией `MirrorCoord`:
      - Для отрицательных индексов: `pos = -pos - 1`.
      - Для индексов за правой/нижней границей: `pos = 2*size - pos - 1`.
    - Результат нормализуется в диапазон [0, 255] с использованием `std::clamp`.
4. Запись результата:
  - Вычисленное значение сохраняется в выходной массив по индексу, рассчитаному через `GetIndex`.

#### 5.2.4. Вспомогательные методы в RunImpl
**GetIndex:**
- Вычисляет линейный индекс пикселя в массиве данных изображения: `index = ((row * width + col) * channels) + channel`.

**MirrorCoord:**
- Реализует метод отражения для обработки границ:
```cpp
size_t MirrorCoord(size_t current, int off, size_t size) {
  int64_t pos = static_cast<int64_t>(current) + off;
  if (pos < 0) return static_cast<size_t>(-pos - 1);
  if (std::cmp_greater_equal(static_cast<size_t>(pos), size)) 
    return (2 * size) - static_cast<size_t>(pos) - 1;
  return static_cast<size_t>(pos);
}
```

#### 5.2.5. PostProcessingImpl
В постпроцессинге нет необходимости, поэтому данный этап пропускается.

### 5.3. Реализация параллельной версии
**Этапы:** `PreProcessingImpl`, `PostProcessingImpl` аналогичны последовательной версии.

#### 5.3.1. Конструктор и инициализация
- **Инициализация MPI окружения:**
  - Определяется ранг текущего процесса и общее количество процессов.
  - Только процесс с рангом 0 загружает входное изображение.

**Реализация на C++:**
```cpp
int proc_rank{};
int proc_num{};
MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
MPI_Comm_size(MPI_COMM_WORLD, &proc_num);
SetTypeOfTask(GetStaticTypeOfTask());

proc_rank_ = static_cast<size_t>(proc_rank);
proc_num_ = static_cast<size_t>(proc_num);
if (proc_rank_ == 0) {
  GetInput() = in;
}
```
#### 5.3.1. ValidationImpl
- Выполняется проверка на процессе ранга 0, аналогична реализации в последовательной версии.
- Результат проверки рассылкается всем процессам через `MPI_Bcast`.
- Таким образом, проверка едина для всех процессов.

#### 5.3.1. RunImpl
- Распределение данных, как описано в разделе [4.1. Распределение данных](#41-распределение-данных).
- Вычисление локальных сумм.
- С помощью `MPI_Allreduce` выполняется:
  - Сбор локальных сумм из процессов.
  - Сложение полученных сумм.
  - Передача результата всем процессам.

**Реализация на C++:**
- Представлена в разделе [Приложение №3 — Параллельная версия решения задачи
](#1032-файл-реализации)

### 5.4. Использование памяти
- Последовательная версия: `O(N)` — входной вектор произвольной длины `N`.
- Параллельная версия: 
  - Ранг 0: `O(N)` — хранит исходный массив + `O(N/P)` — локальная часть данных исходного массива, распределённого `P` процессам.
  - Остальные ранги: `O(N/P)` — локальная часть данных исходного массива, распределённого `P` процессам.

### 5.5. Допущения и крайние случаи
- Все процессы запускаются в рамках одного MPI-коммуникатора.
- Ранг 0 является корневым процессом для распределения данных и валидации.


## 6. Тестовые инфраструктуры
### 6.1. Windows
| Параметр   | Значение                                             |
| ---------- | ---------------------------------------------------- |
| CPU        | Intel Core i5 12400F (6 cores, 12 threads, 2500 MHz) |
| RAM        | 32 GB DDR4 (3200 MHz)                                |
| OS         | Windows 10 (10.0.19045)                              |
| Компилятор | MSVC 19.42.34435, Release Build                      |

### 6.2 WSL
| Параметр   | Значение                                             |
| ---------- | ---------------------------------------------------- |
| CPU        | Intel Core i5 12400F (6 cores, 12 threads, 2500 MHz) |
| RAM        | 16 GB DDR4 (3200 MHz)                                |
| OS         | Ubuntu 24.04.3 LTS on Windows 10 x86_64              |
| Компилятор | GCC 13.3.0, Release Build                            |

### 6.3. Общие настройки
- **Переменные окружения:** PPC_NUM_PROC = 2, 4.
- **Данные:** элементы вектора хранятся в `test_vec*.txt` файлах в директории `./data`.


## 7. Результаты и обсуждение

### 7.1. Корректность

Корректность задачи для обоих версий проверена с помощью набора параметризированных тестов Google Test.

#### 7.1.1. Функциональные тесты
- Вектор с положительными числами:
  - Данные: `./data/test_vec1.txt`.
  - Ожидаемое значение: `50.5`.

- Вектор со смешанными положительными и отрицательными числами:
  - Данные: `./data/test_vec2.txt`.
  - Ожидаемое значение: `14.5`.

- Вектор с одним элементом:
  - Данные: `./data/test_vec_one_elem.txt`.
  - Ожидаемое значение: `5.0`.

- Вектор с дробным средним значением элементов:
  - Данные: `./data/test_vec_fraction.txt`.
  - Ожидаемое значение: `4.0/3.0`.

- Вектор с большого размера:
  - Данные: `./data/test_vec_one_million_elems.txt`.
  - Ожидаемое значение: `-2.60988`.

- Вектор с чередующимися противоположными элементами:
  - Данные: `./data/test_vec_alternating_elems.txt`.
  - Ожидаемое значение: `0.0`.

- Вектор с нулевыми элементами:
  - Данные: `./data/test_vec_zeros_elems.txt`.
  - Ожидаемое значение: `0.0`.

#### 7.1.2. Валидационные тесты
- Обработка пустого вектора:
  - Данные: пустой вектор.
  - Цель: проверка корректной обработки некорректных входных данных.

- Проверка сброса выходного значения перед выполнением:
  - Данные: любой вектор.
  - Цель: проверка провала валидации при изменении состояния переменных до запуска задачи.

#### 7.1.3. Механизм проверки
- Все тесты выполняются как для последовательной (SEQ), так и для параллельной (MPI) версии.
- Данные загружаются из файлов через абсолютные пути, получаемые функцией `GetAbsoluteTaskPath()`.
- Выходное и ожидаемое вещественные значения сравниваются с учётом машинной точности:
    ```с++
    std::fabs(expected_avg_ - output_data) < std::numeric_limits<double>::epsilon()
    ```
    где:
    - `std:fabs()` — вычисляет абсолютное значение разности ожидаемого и полученного значений.
    - `std::numeric_limits<double>::epsilon()` — возвращает машинный эпсилон для типа double.

- Для некорректных сценариев проверяется провал валидации (`ValidationImpl()`).

### 7.2. Производительные тесты

#### 7.2.1. Методология тестирования
- **Данные:** вектор из 1 миллиона элементов дублируется до 256 миллионов. Данные для вектора берутся из файла `./data/test_vec_one_million_elems.txt`.
- **Режимы:**
  - **pipeline** — запуск и измерение времени всех этапов алгоритма (`Validation -> PreProcessing -> Run -> PostProcessing`).
  - **task_run** — запуск всех этапов алгоритма, но измеряется время только на этапе `Run`.
- **Производительность** мерилась только в режиме `task_run`
- **Метрики:** число процессов, абсолютное время выполнения task_run, ускорение, эффективность.

#### 7.2.2. Результаты тестирования на 256 миллионов элементов

**Windows:**
| Режим | Процессов | Время, s | Ускорение | Эффективность |
| ----- | --------- | -------- | --------- | ------------- |
| seq   | 1         | 0.089805 | 1.0000    | N/A           |
| mpi   | 2         | 0.294714 | 0.2476    | 12.4%          |
| mpi   | 4         | 0.427870 | 0.5420    | 13.6%            |

**WSL:**
| Режим | Процессов | Время, s | Ускорение | Эффективность |
| ----- | --------- | -------- | --------- | ------------- |
| seq   | 1         | 0.128304 | 1.0000    | N/A           |
| mpi   | 2         | 0.322329 | 0.3981    | 19.9%         |
| mpi   | 4         | 0.267038 | 0.4805    | 12.1%         |

**\*GitHub:**
| Режим | Процессов | Время, s | Ускорение | Эффективность |
| ----- | --------- | -------- | --------- | ------------- |
| seq   | 1         | 0.057969 | 1.0000    | N/A           |
| mpi   | 2         | 0.181824 | 0.3188    | 15.9%         |

*\*Результаты собирались на локальном форке из Github Actions*

### 7.3. Анализ результатов

- **Аномально низкая производительность:**
  - На объёме данных в 256 миллионов элементов наблюдается значительное замедление MPI-версии по сравнению с последовательной реализацией.
  - На всех тестовых платформах ускорение составляет менее 1 (0.17-0.55).
  - Эффективность падает до значений (12-20%)

- **Сравнение инфраструктур:**
  - Наиболее стабильной и быстрой из тестовых инфраструктур является машина на GitHub.
  - WSL также показывает стабильный результат, но является самой медленной инфраструктурой, что, вероятно, связано архитектурными особенностями технологии.

- **Ограничения масштабируемости:**
  - Из-за затрат на коммуникацию между процессами эффективность значительно снижается.


## 8. Заключения

### 8.1. Достигнутые результаты:
1. **Схема распределения данных** — блочная схема распределения, которая равномерно распределяет данные процессам.
2. **Эффективное использования памяти** — текущая реализация требует хранения части исходного вектора в каждом процессе и целого вектора в процессе ранга 0.
3. **Корректность результатов** — полное соответствие последовательной и параллельной версий.
4. **Модульность и тестируемость** — код структурирован и покрыт функциональными и валидационными тестами.

### 8.2. Выявленные проблемы и возможные улучшения:
1. **Улучшение масштабируемости** — текущая реализация демонстрирует аномально низкую эффективность (12-20%) при работе с большими объёмами данных.
2. **Оптимизация коммуникационных операций** — необходимо исследовать причины значительных накладных расходов на этапе распределения данных входного вектора процессам.
3. **Рассмотрение более эффективных реализаций** — рассмотреть реализации, где минимизированы накладные расходны на передачу данных между процессами.

В рамках данной работы успешно решена задача вычисления среднего арифметического элементов вектора, реализованы два решения: последовательное и параллельное с использованием MPI.

## 9. Источники
1. Расставим точки над структурами C/C++ / Антон Буков // habr.com[сайт] — режим доступа: https://habr.com/ru/articles/142662/ свободный (дата обращения: 10.12.2025).
2. Документация по курсу «Параллельное программирование» // Parallel Programming Course URL: https://learning-process.github.io/parallel_programming_course/ru/index.html (дата обращения: 25.10.2025).
3. std::memcpy // cppreference.com[сайт] — режим доступа: https://en.cppreference.com/w/cpp/string/byte/memcpy свободный (дата обращения: 20.12.2025).
4. "Коллективные и парные взаимодействия" / Сысоев А. В // Лекции по дисциплине «Параллельное программирование для кластерных систем».

## 10. Приложение

### 10.1. Приложение №1 — Общие определения
Файл: `./common/include/common.hpp`.
```cpp

```

### 10.2. Приложение №2 — Последовательная версия решения задачи 
#### 10.2.1. Заголовочный файл:
Файл: `./seq/ops_seq.hpp`.
```cpp

```

#### 10.2.1. Файл реализации:
Файл: `./seq/ops_seq.cpp`.
```cpp

```

### 10.3. Приложение №3 — Параллельная версия решения задачи

#### 10.3.1. Заголовочный файл
Файл: `./mpi/ops_mpi.hpp`.
```cpp

```

#### 10.3.2. Файл реализации
Файл: `./mpi/ops_mpi.cpp`.
```cpp


```

### 10.4. Приложение №4 — функциональные и валидационные тесты
Файл: `./tests/functional/main.cpp`.
```cpp

```

### 10.5. Приложение №5 — проиводительные тесты
Файл: `./tests/performance/main.cpp`.
```cpp


```